{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGenerator:\n",
    "        \n",
    "    def __init__(self,rows,columns,actions):\n",
    "        self.columns = columns\n",
    "        self.rows = rows\n",
    "        self.actions=actions\n",
    "    \n",
    "    \n",
    "    def q_table_3d(self):\n",
    "        raw_maze = np.zeros((self.rows,self.columns,self.actions))\n",
    "        return raw_maze\n",
    "    \n",
    "    \n",
    "    def create_walls(self):\n",
    "        \n",
    "        # tu mu generuje dwie losowe tablice x i y \n",
    "        walls= [[np.random.randint(1,self.rows) for doc in range(2)]for doc in range(self.rows-2) ]\n",
    "        \n",
    "        # To mi skłąda do kupy wszystkie coordynaty które będe wkładał do generatora\n",
    "        walls = list(walls)\n",
    "        for x,y in walls:\n",
    "            raw_maze[x,y] = 3\n",
    "            \n",
    "    \n",
    "    def create_traps(self):\n",
    "        \n",
    "        # tu mu generuje dwie losowe tablice x i y \n",
    "        traps= [[np.random.randint(1,self.rows-2) for doc in range(2)]for doc in range(self.rows-2) ]\n",
    "\n",
    "        \n",
    "        #To mi skłąda do kupy wszystkie coordynaty które będe wkładał do generatora\n",
    "        tarps = list(traps)\n",
    "        for x,y in traps:\n",
    "            raw_maze[x,y] = 4\n",
    "\n",
    "    def finish(self):\n",
    "        raw_maze[self.state,self.state] = 2\n",
    "    \n",
    "    \n",
    "    # RL\n",
    "    def build_qtable(self):\n",
    "        q_table = np.zeros((self.columns *self.rows , self.actions))\n",
    "        return q_table\n",
    "    \n",
    "\n",
    "    \n",
    "    def choose_action(self,state):\n",
    "        \"\"\"choose_action take a random action or take maximal value in the actual state\"\"\"\n",
    "        actions = q_table[x,y,:]\n",
    "        \n",
    "        if (np.random.uniform()>EPSILON) or (actions.all()==0):\n",
    "            action_choose = np.random.choice(4)\n",
    "        else:\n",
    "            action_choose = actions.argmax()\n",
    "        return int(action_choosen)\n",
    "    \n",
    "def randome_arg_max(vector):\n",
    "    \"\"\" Argmax that chooses randomly among eligible maximum indices. \"\"\"\n",
    "    m = np.amax(vector)\n",
    "    indices = np.nonzero(vector == m)[0]\n",
    "    return pr.choice(indices)\n",
    "    \n",
    "def interaction(state,action_choose):\n",
    "    \n",
    "    new_state[0] = state[0]\n",
    "    new_state[1] = state[1]\n",
    "    \n",
    "    if action_choose == 0 : # Right\n",
    "        print('Right')\n",
    "\n",
    "        if  new_state[1] == 4: # Czy jest koniec ściany\n",
    "\n",
    "            new_state[0] = state[0] + 0\n",
    "            new_state[1] = state[1] + 0\n",
    "        else:\n",
    "\n",
    "                \n",
    "            new_state[0] = state[0] + 0\n",
    "            new_state[1] = state[1] + 1\n",
    "            \n",
    "            #win\n",
    "        if new_state[0] == 4 and new_state[1]==4:\n",
    "            print('wiiiiiiiiiiiiiiiiiiiiiiin')\n",
    "            reward = 1\n",
    "            \n",
    "    elif action_choose == 1: # Up\n",
    "        print('UP')\n",
    "            \n",
    "        if  new_state[0]==0 : # czy jest ściana\n",
    "\n",
    "            new_state[0] = state[0] + 0\n",
    "            new_state[1] = state[1] + 0\n",
    "        else:\n",
    "          \n",
    "            new_state[0] = state[0] - 1\n",
    "            new_state[1] = state[1] + 0\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "     \n",
    "            \n",
    "    elif action_choose == 2: # Down\n",
    "        print('Down')\n",
    "        if  new_state[0]== 4:\n",
    "           \n",
    "            new_state[0] = state[0] + 0\n",
    "            new_state[1] = state[1] + 0\n",
    "        else:\n",
    "           \n",
    "            new_state[0] = state[0] + 1\n",
    "            new_state[1] = state[1] + 0\n",
    "        # win\n",
    "        if  new_state[0] == 4 and new_state[1]==4 :\n",
    "            print('wiiiiiiiiiiiiiiiiiiiiiiin')\n",
    "            reward = 1\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "    elif action_choose == 3: # Left\n",
    "        print('Left')\n",
    "        if new_state[1] == 0 :\n",
    "            reward = 0\n",
    "            new_state[0] = state[0] + 0\n",
    "            new_state[1] = state[1] + 0\n",
    "        else:\n",
    "            reward=0\n",
    "            new_state[0] =  state[0] + 0\n",
    "            new_state[1] =  state[0] - 1\n",
    "                \n",
    "    return reward, new_state \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = 5\n",
    "ACTIONS = 4\n",
    "EPSILON = 6.0\n",
    "LEARNING_RATE = 0.7\n",
    "gamma = 0.9\n",
    "DECAY_RATE = 0.01 #exponential decay rate for exploration probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 3\n",
    "\n",
    "rl= MazeGenerator(STATE,STATE,ACTIONS)\n",
    "q_table = rl.q_table_3d()\n",
    "\n",
    "\n",
    "\n",
    "env= np.zeros((5,5,3))\n",
    "env[1,1,1]=1\n",
    "env[1,3,1]=1\n",
    "env[3,3,1]=1\n",
    "env[3,1,1]=1\n",
    "env[4,4,1]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1\n",
      "Qtable actions:  [0. 0. 0. 0.]\n",
      "ACTION CHOOSE:  2\n",
      "Down\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'reward' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-470-b7dd0854fc00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#Interaction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minteraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_choose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#env interaction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-468-70d081e6a2b6>\u001b[0m in \u001b[0;36minteraction\u001b[1;34m(state, action_choose)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mnew_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'reward' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(0,episodes): \n",
    "    print('episode: ', i+1)\n",
    "    terminate = False\n",
    "    state = [0,0]\n",
    "    state[0] = 0\n",
    "    state[1] = 0\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    while not terminate:\n",
    "        \n",
    "        env[state[0],state[1],: ] = 0\n",
    "        \n",
    "        \n",
    "        actions = q_table[state[0],state[1],:]\n",
    "        \n",
    "        \n",
    "        if  (np.random.uniform()>EPSILON) or actions.any()==0.:\n",
    "            action_choose = np.random.choice(4)\n",
    "  \n",
    "        else:\n",
    "            action_choose=randome_arg_max(actions)\n",
    "        print('Qtable actions: ', actions)\n",
    "        print(\"ACTION CHOOSE: \", action_choose)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Interaction\n",
    "        reward, new_state = interaction(state,action_choose)\n",
    "\n",
    "        #env interaction\n",
    "\n",
    "\n",
    "        \n",
    "        #q_table[state[0], state[1], action_choose] = q_table[state[0], state[1], action_choose] * (1 - LEARNING_RATE)+ LEARNING_RATE *(reward +GAMMA * np.max(q_table[new_state[0], new_state[1], :]) )\n",
    "        print('Aktual Q_table: ', q_table[state[0] , state[1] , :])\n",
    "        #q_table[state[0], state[1], action_choose] = q_table[state[0],state[1], action_choose] + LEARNING_RATE*(reward - q_table[new_state[0], new_state[1], action_choose])\n",
    "        \n",
    "        q_table[state[0], state[1], action_choose] = q_table[state[0], state[1], action_choose] + LEARNING_RATE * ( reward + gamma * q_table[new_state[0],new_state[1], action_choose] - q_table[state[0], state[1], action_choose] )\n",
    "        \n",
    "        \n",
    "        env[state[0],state[1],:] = 0\n",
    "        env[new_state[0],new_state[1], 2 ] = 1\n",
    "\n",
    "        state = new_state\n",
    "        \n",
    "        if env[4,4, 2 ] == 1:\n",
    "            env[4,4, 1 ] = 1\n",
    "            terminate = True\n",
    "\n",
    "        plt.imshow(env)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "print(\"qtable\", q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0.7, 0. ]]])"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  ],\n",
       "        [0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  ],\n",
       "        [0.  , 0.13]]])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[1, 1,1] = Q[1,1,1] + 0.1 * ( 0.1+ 0.9 * 0 - 0 )\n",
    "Q     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
